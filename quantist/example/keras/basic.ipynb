{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# tensor\n",
    "\n",
    "t = np.array([[1, 2, 3, 0], [4, 5, 6, 0], [7, 8, 9, 0]])\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 15 18  0]\n"
     ]
    }
   ],
   "source": [
    "sum0 = np.sum(t, axis=0)\n",
    "print(sum0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15 24]\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum(t, axis=1)\n",
    "print(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r    8192/11490434 [..............................] - ETA: 868s"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, imdb, reuters, cifar10, cifar100, boston_housing\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "#                                                       nb_words=None,\n",
    "#                                                       skip_top=0,\n",
    "#                                                       maxlen=None,\n",
    "#                                                       test_split=0.1)\n",
    "#                                                       seed=113,\n",
    "#                                                       start_char=1,\n",
    "#                                                       oov_char=2,\n",
    "#                                                       index_from=3)\n",
    "# (X_train, y_train), (X_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "#                                                          nb_words=None,\n",
    "#                                                          skip_top=0,\n",
    "#                                                          maxlen=None,\n",
    "#                                                          test_split=0.2,\n",
    "#                                                          seed=113,\n",
    "#                                                          start_char=1,\n",
    "#                                                          oov_char=2,\n",
    "#                                                          index_from=3)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "# \n",
    "# (x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Reshape\n",
    "\n",
    "# model.add(Dense(nb_classes, input_shape=(784,)))#全连接，输入784维度, 输出10维度，需要和输入输出对应\n",
    "\n",
    "\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(200, input_shape=(784,)))#全连接，输入784维度, 输出10维度，需要和输入输出对应\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(100))# 除了首层需要设置输入维度，其他层只需要输入输出维度就可以了，输入维度自动继承上层。\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(60))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(30))            #model.add(Activation('sigmoid'))和model.add(Dense(30))可以合并写出\n",
    "# model.add(Activation('sigmoid'))#model.add(Dense(30,activation='softmax'))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "# \tsgd = Adam(lr=0.003)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "# \t#model 概要\n",
    "# model.summary()\n",
    "\n",
    "# \t____________________________________________________________________________________________________\n",
    "# Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "# ====================================================================================================\n",
    "# dense_23 (Dense)                 (None, 200)           157000      dense_input_7[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_23 (Activation)       (None, 200)           0           dense_23[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_24 (Dense)                 (None, 100)           20100       activation_23[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_24 (Activation)       (None, 100)           0           dense_24[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_25 (Dense)                 (None, 60)            6060        activation_24[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_25 (Activation)       (None, 60)            0           dense_25[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_26 (Dense)                 (None, 30)            1830        activation_25[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_26 (Activation)       (None, 30)            0           dense_26[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_27 (Dense)                 (None, 10)            310         activation_26[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_27 (Activation)       (None, 10)            0           dense_27[0][0]                   \n",
    "# ====================================================================================================\n",
    "# Total params: 185,300\n",
    "# Trainable params: 185,300\n",
    "# Non-trainable params: 0\n",
    "# ____________________________________________________________________________________________________\n",
    "# \tSVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# \ttensorboard2 = TensorBoard(log_dir='/home/tensorflow/log/five_layer_sigmoid/epoch', histogram_freq=0)\n",
    "# my_tensorboard2 = BatchTensorBoard(log_dir='/home/tensorflow/log/five_layer_sigmoid/batch')\n",
    "# model.fit(x_train_1, y_train_1,\n",
    "#           nb_epoch=20,\n",
    "#           verbose=0,\n",
    "#           batch_size=100,\n",
    "#           callbacks=[my_tensorboard2, tensorboard2])\n",
    "# \t<keras.callbacks.History at 0xf868a90>\n",
    "# \t#模型的测试误差指标\n",
    "# print(model.metrics_names)\n",
    "# # 对测试数据进行测试\n",
    "# model.evaluate(x_test_1, y_test_1,\n",
    "#           verbose=1,\n",
    "#           batch_size=100)\n",
    "# \t['loss', 'acc']\n",
    "#  9800/10000 [============================>.] - ETA: 0s\n",
    "# [0.036339853547979147, 0.98736999988555907]\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(200, input_shape=(784,)))#全连接，输入784维度, 输出10维度，需要和输入输出对应\n",
    "# model.add(Activation('relu'))# 将激活函数sigmoid改为ReLU\n",
    "# model.add(Dense(100))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(60))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(30))            \n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "# \tsgd = Adam(lr=0.001)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "# \t#model 概要\n",
    "# model.summary()\n",
    "# \t____________________________________________________________________________________________________\n",
    "# Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "# ====================================================================================================\n",
    "# dense_16 (Dense)                 (None, 200)           157000      dense_input_4[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_16 (Activation)       (None, 200)           0           dense_16[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_17 (Dense)                 (None, 100)           20100       activation_16[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_17 (Activation)       (None, 100)           0           dense_17[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_18 (Dense)                 (None, 60)            6060        activation_17[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_18 (Activation)       (None, 60)            0           dense_18[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_19 (Dense)                 (None, 30)            1830        activation_18[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_19 (Activation)       (None, 30)            0           dense_19[0][0]                   \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_20 (Dense)                 (None, 10)            310         activation_19[0][0]              \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_20 (Activation)       (None, 10)            0           dense_20[0][0]                   \n",
    "# ====================================================================================================\n",
    "# Total params: 185,300\n",
    "# Trainable params: 185,300\n",
    "# Non-trainable params: 0\n",
    "# ____________________________________________________________________________________________________\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# tensorboard3 = TensorBoard(log_dir='/home/tensorflow/log/five_layer_relu/epoch', histogram_freq=0)\n",
    "# my_tensorboard3 = BatchTensorBoard(log_dir='/home/tensorflow/log/five_layer_relu/batch')\n",
    "# model.fit(x_train_1, y_train_1,\n",
    "#           nb_epoch=30,\n",
    "#           verbose=0,\n",
    "#           batch_size=100,\n",
    "#           callbacks=[my_tensorboard3, tensorboard3])\n",
    "# \t<keras.callbacks.History at 0xe3c6d50>\n",
    "# \t#模型的测试误差指标\n",
    "# print(model.metrics_names)\n",
    "# # 对测试数据进行测试\n",
    "# model.evaluate(x_test_1, y_test_1,\n",
    "#           verbose=1,\n",
    "#           batch_size=100)\n",
    "# \t['loss', 'acc']\n",
    "#  9600/10000 [===========================>..] - ETA: 0s\n",
    "# [0.017244604945910281, 0.99598000288009647]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dropout------------------------------\n",
    "# model = Sequential()\n",
    "# model.add(Dense(200, input_shape=(784,)))#全连接，输入784维度, 输出10维度，需要和输入输出对应\n",
    "# model.add(Activation('relu'))# 将激活函数sigmoid改为ReLU\n",
    "# model.add(Dense(100))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.25))# 添加一个dropout层, 随机移除25%的单元\n",
    "# model.add(Dense(60))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(30))            \n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "# \tsgd = Adam(lr=0.001)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "# \t#model 概要\n",
    "# model.summary()\n",
    "# \t____________________________________________________________________________________________________\n",
    "# Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "# ====================================================================================================\n",
    "# dense_171 (Dense)                (None, 200)           157000      dense_input_35[0][0]             \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_171 (Activation)      (None, 200)           0           dense_171[0][0]                  \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_172 (Dense)                (None, 100)           20100       activation_171[0][0]             \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_172 (Activation)      (None, 100)           0           dense_172[0][0]                  \n",
    "# ____________________________________________________________________________________________________\n",
    "# dropout_100 (Dropout)            (None, 100)           0           activation_172[0][0]             \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_173 (Dense)                (None, 60)            6060        dropout_100[0][0]                \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_173 (Activation)      (None, 60)            0           dense_173[0][0]                  \n",
    "# ____________________________________________________________________________________________________\n",
    "# dropout_101 (Dropout)            (None, 60)            0           activation_173[0][0]             \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_174 (Dense)                (None, 30)            1830        dropout_101[0][0]                \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_174 (Activation)      (None, 30)            0           dense_174[0][0]                  \n",
    "# ____________________________________________________________________________________________________\n",
    "# dropout_102 (Dropout)            (None, 30)            0           activation_174[0][0]             \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_175 (Dense)                (None, 10)            310         dropout_102[0][0]                \n",
    "# ____________________________________________________________________________________________________\n",
    "# activation_175 (Activation)      (None, 10)            0           dense_175[0][0]                  \n",
    "# ====================================================================================================\n",
    "# Total params: 185,300\n",
    "# Trainable params: 185,300\n",
    "# Non-trainable params: 0\n",
    "# ____________________________________________________________________________________________________\n",
    "# \tSVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# \ttensorboard4 = TensorBoard(log_dir='/home/tensorflow/log/five_layer_relu_dropout/epoch')\n",
    "# my_tensorboard4 = BatchTensorBoard(log_dir='/home/tensorflow/log/five_layer_relu_dropout/batch')\n",
    "# \tmodel.fit(x_train_1, y_train_1,\n",
    "#           nb_epoch=30,\n",
    "#           verbose=0,\n",
    "#           batch_size=100,\n",
    "#           callbacks=[tensorboard4, my_tensorboard4])\n",
    "# \t<keras.callbacks.History at 0x27819610>\n",
    "# \t#模型的测试误差指标\n",
    "# print(model.metrics_names)\n",
    "# # 对测试数据进行测试\n",
    "# model.evaluate(x_test_1, y_test_1,\n",
    "#           verbose=1,\n",
    "#           batch_size=100)\n",
    "# \t['loss', 'acc']\n",
    "#  9900/10000 [============================>.] - ETA: 0s\n",
    "# [0.025450729207368569, 0.99462999999523161]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况从而无法完成深层网络的训练。\n",
    "# 在sigmoid接近饱和区时，变换非常缓慢，导数趋于0，减缓收敛速度。\n",
    "# ReLU具有线性、非饱和性，而其非饱和性使得网络可以自行引入稀疏性。\n",
    "# ReLU在训练时是非常脆弱的，并且可能会“死”。而设置一个适当的学习率，可以在一定程度上避免这一问题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam, TFOptimizer \n",
    "\n",
    "# all_classes = {\n",
    "#         'sgd': SGD,\n",
    "#         'rmsprop': RMSprop,\n",
    "#         'adagrad': Adagrad,\n",
    "#         'adadelta': Adadelta,\n",
    "#         'adam': Adam,\n",
    "#         'adamax': Adamax,\n",
    "#         'nadam': Nadam,\n",
    "#         'tfoptimizer': TFOptimizer,\n",
    "#     }\n",
    "\n",
    "# sgd = SGD(lr=0.005)\n",
    "# #binary_crossentropy，就是交叉熵函数\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 交叉熵（cross-entropy）就是神经网络中常用的损失函数。一个比较简单的理解就是使得 预测值Yi和真实值Y' 对接近，即两者的乘积越大，coss-entropy越小\n",
    "# 交叉熵和准确度变化图像可以看 TensorBoard 。\n",
    "\n",
    "# #模型的测试误差指标\n",
    "# print(model.metrics_names)\n",
    "# # 对测试数据进行测试\n",
    "# model.evaluate(x_test_1, y_test_1,\n",
    "#           verbose=1,\n",
    "#           batch_size=100)\n",
    "# \t['loss', 'acc']\n",
    "#  9800/10000 [============================>.] - ETA: 0s\n",
    "# [0.87580669939517974, 0.94387999653816224]\n",
    "\n",
    "# 学习速率大小的调节一般取决于 loss 的变化幅度。\n",
    "\n",
    "\n",
    "\n",
    "# sigmod有个缺点，sigmoid函数反向传播时，很容易就会出现梯度消失,在接近饱和区的时候，导数趋向0，会变得非常缓慢。\n",
    "# 因此，在优化器选择时选用Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback, TensorBoard\n",
    "# tensorboard = TensorBoard(log_dir='/home/tensorflow/log/softmax/epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
